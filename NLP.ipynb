{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove noisy words from a text\n",
    "\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove a regex pattern \n",
    "import re \n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown locale: UTF-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82bd39ba37c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample code to perform stemming and lemmatisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[1;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParserI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/chunk/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_2_unicode_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m           \u001b[0;32mimport\u001b[0m \u001b[0mTaggerI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m          \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple2str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m from nltk.tag.sequential    import (SequentialBackoffTagger, ContextTagger,\n\u001b[0m\u001b[1;32m     71\u001b[0m                                     \u001b[0mDefaultTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNgramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnigramTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                     \u001b[0mBigramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrigramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAffixTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_2_unicode_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/classify/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                                   ConditionalExponentialClassifier)\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSenna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextcat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextCat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/classify/textcat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# You may have to \"pip install regx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/regex.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;31m# We define _pattern_type here after all the support objects have been defined.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m \u001b[0m_pattern_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;31m# We'll define an alias for the 'compile' function so that the repr of a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/regex.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags, kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_locale_sensitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mLOCALE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# This pattern is, or might be, locale-sensitive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mpattern_locale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# This pattern is definitely not locale-sensitive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/locale.py\u001b[0m in \u001b[0;36mgetlocale\u001b[0;34m(category)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLC_ALL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category LC_ALL is not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/locale.py\u001b[0m in \u001b[0;36m_parse_localename\u001b[0;34m(localename)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown locale: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_build_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocaletuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown locale: UTF-8"
     ]
    }
   ],
   "source": [
    "# Sample code to perform stemming and lemmatisation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "print(lem.lemmatize(word, \"v\"))\n",
    " \n",
    "print(stem.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code to do object standardisation\n",
    "\n",
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal, dm akansha luv movie was awsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample code to do POS\n",
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code to do topic modelling using LDA\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code to generate bigram of a text\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "generate_ngrams('this is a sample text', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code to generate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print (model.similarity('data', 'science'))\n",
    "print('******************')\n",
    "print (model['learning']  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code for text classification using textblob and naive bayes classifier\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "\n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "\n",
    "print(model.accuracy(test_corpus))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code for text classification using sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "\n",
    "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "print (classification_report(test_labels, prediction))\n",
    "\n",
    "test_sentence=vectorizer.transform(['Hi my name is akansha'])\n",
    "model.predict(test_sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code to implement Levenshtein Distance\n",
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Sample code to compute soundex phonetics strings for different words\n",
    "import phonetics as p\n",
    "print(p.soundex('akansha'))\n",
    "print(p.soundex('akanksha'))\n",
    "print(p.soundex('aakansha'))\n",
    "print(p.soundex('aakanksha'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code to implement cosine similarity\n",
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
